Epoch 1914: LambdaDecay set learning rate to 1.2242284984610906e-07.
===========>   training    <===========
Epoch: [1914][0/6]      Lr: [1.2242284984610906e-07]    Loss 0.6454 (0.6454)
0.9985464 0.00020384423
===========>   testing    <===========
Epoch: [1914][0/289]    Lr: [1.2242284984610906e-07]    Loss 0.5402 (0.5402)
0.99242556 1.40116345e-08
Epoch: [1914][100/289]  Lr: [1.2242284984610906e-07]    Loss 0.5117 (0.6279)
0.99368024 0.00045070678
Epoch: [1914][200/289]  Lr: [1.2242284984610906e-07]    Loss 0.6610 (0.6287)
0.9859342 6.316597e-05
loss:  0.8631777205995061 0.7166599564597753
Epoch 1915: LambdaDecay set learning rate to 1.2242284984610906e-07.
===========>   training    <===========
Epoch: [1915][0/6]      Lr: [1.2242284984610906e-07]    Loss 0.6482 (0.6482)
0.9737699 0.0019470582
===========>   testing    <===========
Epoch: [1915][0/289]    Lr: [1.2242284984610906e-07]    Loss 0.5618 (0.5618)
0.9935028 9.281216e-12
Epoch: [1915][100/289]  Lr: [1.2242284984610906e-07]    Loss 0.5226 (0.6326)
0.99385405 0.0036869314
Epoch: [1915][200/289]  Lr: [1.2242284984610906e-07]    Loss 0.6830 (0.6313)
0.97872865 0.0020902806
loss:  0.8612701647270633 0.7166599564597753
Epoch 1916: LambdaDecay set learning rate to 1.2242284984610906e-07.
===========>   training    <===========
Epoch: [1916][0/6]      Lr: [1.2242284984610906e-07]    Loss 0.6370 (0.6370)
0.98649186 0.0015597711
===========>   testing    <===========
Epoch: [1916][0/289]    Lr: [1.2242284984610906e-07]    Loss 0.5197 (0.5197)
0.99344385 1.3425235e-09
Epoch: [1916][100/289]  Lr: [1.2242284984610906e-07]    Loss 0.4845 (0.6341)
0.99905366 6.8684905e-08
Epoch: [1916][200/289]  Lr: [1.2242284984610906e-07]    Loss 0.6757 (0.6312)
0.9915268 0.0011904179
loss:  0.860866812294701 0.7166599564597753
Epoch 1917: LambdaDecay set learning rate to 1.2242284984610906e-07.
===========>   training    <===========
Epoch: [1917][0/6]      Lr: [1.2242284984610906e-07]    Loss 0.6470 (0.6470)
0.9907589 0.006077458
===========>   testing    <===========
Epoch: [1917][0/289]    Lr: [1.2242284984610906e-07]    Loss 0.5658 (0.5658)
0.99395305 3.819764e-12
Epoch: [1917][100/289]  Lr: [1.2242284984610906e-07]    Loss 0.5570 (0.6317)
0.99587363 0.0007844118
Epoch: [1917][200/289]  Lr: [1.2242284984610906e-07]    Loss 0.6642 (0.6326)
0.97785884 0.0023838116
loss:  0.8658055742696869 0.7166599564597753
Epoch 1918: LambdaDecay set learning rate to 1.2242284984610906e-07.
===========>   training    <===========
Epoch: [1918][0/6]      Lr: [1.2242284984610906e-07]    Loss 0.6385 (0.6385)
0.9953353 1.0436266e-07
===========>   testing    <===========
Epoch: [1918][0/289]    Lr: [1.2242284984610906e-07]    Loss 0.5547 (0.5547)
0.99612916 3.157846e-09
Epoch: [1918][100/289]  Lr: [1.2242284984610906e-07]    Loss 0.5474 (0.6337)
0.99463993 0.0017586664
Epoch: [1918][200/289]  Lr: [1.2242284984610906e-07]    Loss 0.6620 (0.6318)
0.9810255 0.0022568046
loss:  0.8590915997458695 0.7166599564597753
Epoch 1919: LambdaDecay set learning rate to 1.2242284984610906e-07.
===========>   training    <===========
Epoch: [1919][0/6]      Lr: [1.2242284984610906e-07]    Loss 0.6301 (0.6301)
0.9734124 4.774665e-06
===========>   testing    <===========
Epoch: [1919][0/289]    Lr: [1.2242284984610906e-07]    Loss 0.5443 (0.5443)
0.9940725 1.0353915e-13
Epoch: [1919][100/289]  Lr: [1.2242284984610906e-07]    Loss 0.5226 (0.6292)
0.9926454 0.0011118153
Epoch: [1919][200/289]  Lr: [1.2242284984610906e-07]    Loss 0.6849 (0.6279)
0.98881185 0.0029048615
loss:  0.8621891449197382 0.7166599564597753
Epoch 1920: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1920][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6363 (0.6363)
0.9942087 6.375774e-36
===========>   testing    <===========
Epoch: [1920][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5691 (0.5691)
0.99203074 1.0505177e-13
Epoch: [1920][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5021 (0.6280)
0.99326235 9.14332e-05
Epoch: [1920][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6743 (0.6278)
0.9925662 0.00033797813
loss:  0.8654681231724706 0.7166599564597753
Epoch 1921: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1921][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6311 (0.6311)
0.97159654 0.0018381911
===========>   testing    <===========
Epoch: [1921][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5581 (0.5581)
0.9939447 2.5413591e-19
Epoch: [1921][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5213 (0.6352)
0.9960246 0.0008161616
Epoch: [1921][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6708 (0.6331)
0.9712542 0.0023096388
loss:  0.865497159984566 0.7166599564597753
Epoch 1922: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1922][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6291 (0.6291)
0.9956317 0.00018161297
===========>   testing    <===========
Epoch: [1922][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5435 (0.5435)
0.99670106 1.0463228e-06
Epoch: [1922][100/289]  Lr: [1.163017073538036e-07]     Loss 0.4961 (0.6327)
0.9995677 2.0244914e-05
Epoch: [1922][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6794 (0.6315)
0.97501886 0.0024354428
loss:  0.8613839069351662 0.7166599564597753
Epoch 1923: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1923][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6288 (0.6288)
0.993111 0.00017083774
===========>   testing    <===========
Epoch: [1923][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5584 (0.5584)
0.99310416 9.4849455e-08
Epoch: [1923][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5492 (0.6324)
0.99593383 0.0011045354
Epoch: [1923][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6773 (0.6312)
0.9741513 0.0027306054
loss:  0.86284598677569 0.7166599564597753
Epoch 1924: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1924][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6416 (0.6416)
0.99816424 6.0829425e-30
===========>   testing    <===========
Epoch: [1924][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5402 (0.5402)
0.99667054 1.5372099e-06
Epoch: [1924][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5226 (0.6324)
0.99441904 0.0013024345
Epoch: [1924][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6744 (0.6321)
0.97791606 0.0018584365
loss:  0.8670385459756932 0.7166599564597753
Epoch 1925: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1925][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6422 (0.6422)
0.9708047 0.005088653
===========>   testing    <===========
Epoch: [1925][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5419 (0.5419)
0.99575007 2.4107406e-08
Epoch: [1925][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5116 (0.6330)
0.9937463 0.00018216536
Epoch: [1925][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6750 (0.6319)
0.98377544 0.002003219
loss:  0.8652526422714331 0.7166599564597753
Epoch 1926: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1926][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6453 (0.6453)
1.0 5.102299e-39
===========>   testing    <===========
Epoch: [1926][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5464 (0.5464)
0.9959305 1.6962793e-05
Epoch: [1926][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5194 (0.6270)
0.99451053 0.0013453631
Epoch: [1926][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6993 (0.6261)
0.98499125 8.30328e-06
loss:  0.870430309166176 0.7166599564597753
Epoch 1927: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1927][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6532 (0.6532)
0.994399 0.00048855145
===========>   testing    <===========
Epoch: [1927][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5451 (0.5451)
0.99384093 1.047649e-08
Epoch: [1927][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5236 (0.6295)
0.9931632 0.0024435704
Epoch: [1927][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6809 (0.6295)
0.99320203 0.00034029206
loss:  0.857336190654635 0.7166599564597753
Epoch 1928: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1928][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6388 (0.6388)
0.9984048 0.00013142696
===========>   testing    <===========
Epoch: [1928][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5676 (0.5676)
0.9826319 8.8935017e-13
Epoch: [1928][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5456 (0.6332)
0.994304 7.799202e-05
Epoch: [1928][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6670 (0.6322)
0.98684317 0.008483261
loss:  0.8713958062072253 0.7166599564597753
Epoch 1929: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1929][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6320 (0.6320)
0.9999888 4.8724964e-38
===========>   testing    <===========
Epoch: [1929][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5563 (0.5563)
0.993679 1.2459344e-17
Epoch: [1929][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5016 (0.6297)
0.99356395 0.00019721249
Epoch: [1929][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6802 (0.6293)
0.9800598 0.002627737
loss:  0.8676305581277544 0.7166599564597753
Epoch 1930: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1930][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6410 (0.6410)
0.9921021 0.0003600042
===========>   testing    <===========
Epoch: [1930][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5392 (0.5392)
0.9937085 3.0481577e-09
Epoch: [1930][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5169 (0.6274)
0.9931117 0.00047718995
Epoch: [1930][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6849 (0.6272)
0.97664183 0.0022719773
loss:  0.8559433239524541 0.7166599564597753
Epoch 1931: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1931][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6294 (0.6294)
0.99068636 0.0034597407
===========>   testing    <===========
Epoch: [1931][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5658 (0.5658)
0.99282575 5.3770264e-17
Epoch: [1931][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5253 (0.6346)
0.9915447 0.0006193822
Epoch: [1931][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6785 (0.6327)
0.9731717 0.008084024
loss:  0.8635015156349147 0.7166599564597753
Epoch 1932: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1932][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6364 (0.6364)
0.9999999 7.412754e-27
===========>   testing    <===========
Epoch: [1932][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5393 (0.5393)
0.994553 5.46954e-07
Epoch: [1932][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5135 (0.6289)
0.99408275 0.0010574885
Epoch: [1932][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6965 (0.6280)
0.992215 0.0034265073
loss:  0.8638655399772085 0.7166599564597753
Epoch 1933: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1933][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6351 (0.6351)
0.97354543 0.0025249356
===========>   testing    <===========
Epoch: [1933][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5517 (0.5517)
0.9944629 8.1271754e-14
Epoch: [1933][100/289]  Lr: [1.163017073538036e-07]     Loss 0.4976 (0.6355)
0.99746907 3.361993e-07
Epoch: [1933][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6783 (0.6343)
0.99137455 5.4440075e-05
loss:  0.8573092136642884 0.7166599564597753
Epoch 1934: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1934][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6245 (0.6245)
0.9755347 0.002687819
===========>   testing    <===========
Epoch: [1934][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5400 (0.5400)
0.9941955 1.3920558e-08
Epoch: [1934][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5310 (0.6334)
0.99339044 0.00072076306
Epoch: [1934][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6613 (0.6327)
0.9811147 0.0017596303
loss:  0.8618996924321444 0.7166599564597753
Epoch 1935: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1935][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6357 (0.6357)
0.97525114 0.0010206092
===========>   testing    <===========
Epoch: [1935][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5257 (0.5257)
0.9956136 1.3134152e-11
Epoch: [1935][100/289]  Lr: [1.163017073538036e-07]     Loss 0.4879 (0.6289)
0.9946419 0.00055962056
Epoch: [1935][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6753 (0.6280)
0.9831473 0.0017281543
loss:  0.8616876443244539 0.7166599564597753
Epoch 1936: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1936][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6329 (0.6329)
0.97291136 0.0030632657
===========>   testing    <===========
Epoch: [1936][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5568 (0.5568)
0.99315757 0.000109964494
Epoch: [1936][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5506 (0.6302)
0.99394304 0.0006050239
Epoch: [1936][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6728 (0.6309)
0.9784923 0.0018779375
loss:  0.8716903572849207 0.7166599564597753
Epoch 1937: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1937][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6338 (0.6338)
0.9892582 0.0012699545
===========>   testing    <===========
Epoch: [1937][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5409 (0.5409)
0.99574345 1.32358825e-14
Epoch: [1937][100/289]  Lr: [1.163017073538036e-07]     Loss 0.4972 (0.6269)
0.9928323 0.001023254
Epoch: [1937][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6605 (0.6258)
0.9754397 0.0018347664
loss:  0.8669237838789894 0.7166599564597753
Epoch 1938: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1938][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6281 (0.6281)
0.9955011 4.116758e-22
===========>   testing    <===========
Epoch: [1938][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5300 (0.5300)
0.9960542 2.6876217e-08
Epoch: [1938][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5211 (0.6315)
0.9938279 8.6261665e-05
Epoch: [1938][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6830 (0.6293)
0.9818149 0.002372244
loss:  0.8632365543495415 0.7166599564597753
Epoch 1939: LambdaDecay set learning rate to 1.163017073538036e-07.
===========>   training    <===========
Epoch: [1939][0/6]      Lr: [1.163017073538036e-07]     Loss 0.6383 (0.6383)
0.9946467 4.4808712e-10
===========>   testing    <===========
Epoch: [1939][0/289]    Lr: [1.163017073538036e-07]     Loss 0.5684 (0.5684)
0.9958211 0.0003579343
Epoch: [1939][100/289]  Lr: [1.163017073538036e-07]     Loss 0.5268 (0.6346)
0.9946957 0.00069682713
Epoch: [1939][200/289]  Lr: [1.163017073538036e-07]     Loss 0.6828 (0.6357)
0.9793382 0.0019433365
loss:  0.8618828694261782 0.7166599564597753
Epoch 1940: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1940][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6279 (0.6279)
0.99394035 0.0036538702
===========>   testing    <===========
Epoch: [1940][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5549 (0.5549)
0.9959233 1.1652395e-09
Epoch: [1940][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.4805 (0.6325)
0.9937011 0.0010289312
Epoch: [1940][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6784 (0.6321)
0.97613096 0.002456913
loss:  0.8707405524985763 0.7166599564597753
Epoch 1941: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1941][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6376 (0.6376)
0.9733368 0.0006204714
===========>   testing    <===========
Epoch: [1941][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5379 (0.5379)
0.9953419 3.150243e-12
Epoch: [1941][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.4987 (0.6320)
0.9931954 0.0022781205
Epoch: [1941][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6717 (0.6302)
0.99079585 0.0023811655
loss:  0.8629096658397026 0.7166599564597753
Epoch 1942: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1942][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6465 (0.6465)
0.99345607 6.739484e-08
===========>   testing    <===========
Epoch: [1942][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5657 (0.5657)
0.9938704 2.6132365e-09
Epoch: [1942][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5317 (0.6370)
0.9942266 0.000381732
Epoch: [1942][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6619 (0.6348)
0.98658955 0.0023655656
loss:  0.8626833986779497 0.7166599564597753
Epoch 1943: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1943][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6514 (0.6514)
0.99554765 1.1586074e-26
===========>   testing    <===========
Epoch: [1943][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5604 (0.5604)
0.9929334 5.1450416e-10
Epoch: [1943][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5189 (0.6297)
0.992784 0.0028160412
Epoch: [1943][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6814 (0.6305)
0.9924706 0.0018799996
loss:  0.8695187014513905 0.7166599564597753
Epoch 1944: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1944][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6249 (0.6249)
0.9940514 5.9149177e-05
===========>   testing    <===========
Epoch: [1944][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5484 (0.5484)
0.99344134 4.7349076e-11
Epoch: [1944][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5439 (0.6337)
0.9951715 0.002
Epoch: [1944][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6594 (0.6324)
0.99079937 0.0030263558
loss:  0.8640350169474601 0.7166599564597753
Epoch 1945: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1945][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6512 (0.6512)
0.9941398 0.0021159046
===========>   testing    <===========
Epoch: [1945][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5640 (0.5640)
0.9924076 4.2149906e-05
Epoch: [1945][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5157 (0.6319)
0.99349654 0.00087890757
Epoch: [1945][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6754 (0.6303)
0.9722419 0.0023124702
loss:  0.8587096845808841 0.7166599564597753
Epoch 1946: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1946][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6329 (0.6329)
0.9998652 0.00024705825
===========>   testing    <===========
Epoch: [1946][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5522 (0.5522)
0.9945773 3.5159826e-14
Epoch: [1946][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5012 (0.6342)
0.99557555 0.0017499984
Epoch: [1946][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6855 (0.6324)
0.99060434 0.008297318
loss:  0.866229069791548 0.7166599564597753
Epoch 1947: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1947][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6307 (0.6307)
0.99206394 0.0016282666
===========>   testing    <===========
Epoch: [1947][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5593 (0.5593)
0.99246746 2.0306763e-10
Epoch: [1947][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5261 (0.6302)
0.9925685 0.00050518947
Epoch: [1947][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6710 (0.6303)
0.99255073 0.002944587
loss:  0.863420333620431 0.7166599564597753
Epoch 1948: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1948][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6275 (0.6275)
0.99497765 0.0006709743
===========>   testing    <===========
Epoch: [1948][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5625 (0.5625)
0.9924844 2.3527011e-15
Epoch: [1948][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5094 (0.6297)
0.99315053 0.0011202171
Epoch: [1948][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6631 (0.6284)
0.9789722 0.0028561507
loss:  0.8617898561302819 0.7166599564597753
Epoch 1949: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1949][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6329 (0.6329)
0.99969566 1.3127005e-06
===========>   testing    <===========
Epoch: [1949][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5520 (0.5520)
0.9916431 1.6219744e-12
Epoch: [1949][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5061 (0.6238)
0.9933084 0.0026229785
Epoch: [1949][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6737 (0.6261)
0.99246275 0.0014922944
loss:  0.8615574604091549 0.7166599564597753
Epoch 1950: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1950][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6446 (0.6446)
0.9703171 5.2179963e-05
===========>   testing    <===========
Epoch: [1950][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5549 (0.5549)
0.99084514 2.1607782e-05
Epoch: [1950][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5266 (0.6262)
0.9932266 0.0015791375
Epoch: [1950][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6628 (0.6260)
0.9746128 0.0027444006
loss:  0.8597068646834194 0.7166599564597753
Epoch 1951: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1951][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6312 (0.6312)
0.9789625 0.0027276159
===========>   testing    <===========
Epoch: [1951][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5471 (0.5471)
0.9958752 3.6884243e-10
Epoch: [1951][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5076 (0.6249)
0.9973272 8.8200295e-06
Epoch: [1951][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6614 (0.6249)
0.9758185 0.0024220666
loss:  0.8673623270302802 0.7166599564597753
Epoch 1952: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1952][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6372 (0.6372)
0.994899 4.4915277e-31
===========>   testing    <===========
Epoch: [1952][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5599 (0.5599)
0.99449843 2.0992915e-15
Epoch: [1952][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5484 (0.6368)
0.99376833 0.00063262804
Epoch: [1952][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6843 (0.6354)
0.97506374 0.0024114007
loss:  0.8598058342382698 0.7166599564597753
Epoch 1953: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1953][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6340 (0.6340)
0.9999808 0.0
===========>   testing    <===========
Epoch: [1953][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5638 (0.5638)
0.99363905 1.1800245e-22
Epoch: [1953][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5170 (0.6354)
0.99270624 0.001737988
Epoch: [1953][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6862 (0.6340)
0.98049265 0.0032110587
loss:  0.8690728633582158 0.7166599564597753
Epoch 1954: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1954][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6394 (0.6394)
0.97498816 0.0026981938
===========>   testing    <===========
Epoch: [1954][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5383 (0.5383)
0.9934362 7.8964735e-14
Epoch: [1954][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5213 (0.6322)
0.9935016 0.00014360958
Epoch: [1954][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6770 (0.6309)
0.9854554 0.0024951848
loss:  0.8650292986097855 0.7166599564597753
Epoch 1955: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1955][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6354 (0.6354)
0.9959305 6.5746286e-20
===========>   testing    <===========
Epoch: [1955][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5505 (0.5505)
0.9931492 3.524726e-17
Epoch: [1955][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5269 (0.6315)
0.9952911 0.00024926176
Epoch: [1955][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6938 (0.6292)
0.98207927 0.0048282226
loss:  0.8599234649476262 0.7166599564597753
Epoch 1956: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1956][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6318 (0.6318)
0.9943691 0.00030612602
===========>   testing    <===========
Epoch: [1956][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5449 (0.5449)
0.9942675 4.1000775e-17
Epoch: [1956][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5085 (0.6285)
0.99449486 0.0022280568
Epoch: [1956][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6833 (0.6282)
0.9775569 0.002491482
loss:  0.8703545827688375 0.7166599564597753
Epoch 1957: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1957][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6379 (0.6379)
0.99907 0.00012692653
===========>   testing    <===========
Epoch: [1957][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5366 (0.5366)
0.9955688 1.14772234e-07
Epoch: [1957][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5094 (0.6285)
0.99312097 0.001835257
Epoch: [1957][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6584 (0.6272)
0.9927977 0.00025708464
loss:  0.8580816728589717 0.7166599564597753
Epoch 1958: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1958][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6361 (0.6361)
0.99998844 0.0
===========>   testing    <===========
Epoch: [1958][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5336 (0.5336)
0.9940811 7.908751e-08
Epoch: [1958][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.5106 (0.6281)
0.9944424 0.0022356876
Epoch: [1958][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6535 (0.6278)
0.9797718 0.0019013109
loss:  0.8621412735942593 0.7166599564597753
Epoch 1959: LambdaDecay set learning rate to 1.1048662198611342e-07.
===========>   training    <===========
Epoch: [1959][0/6]      Lr: [1.1048662198611342e-07]    Loss 0.6401 (0.6401)
0.97584754 0.0012182803
===========>   testing    <===========
Epoch: [1959][0/289]    Lr: [1.1048662198611342e-07]    Loss 0.5401 (0.5401)
0.99513215 4.380969e-07
Epoch: [1959][100/289]  Lr: [1.1048662198611342e-07]    Loss 0.4854 (0.6284)
0.9994567 1.4845302e-05
Epoch: [1959][200/289]  Lr: [1.1048662198611342e-07]    Loss 0.6845 (0.6276)
0.9815338 0.0040518628
loss:  0.8599146007363182 0.7166599564597753
Epoch 1960: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1960][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6345 (0.6345)
0.9703081 0.00330031
===========>   testing    <===========
Epoch: [1960][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5413 (0.5413)
0.9934483 1.1391969e-07
Epoch: [1960][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5058 (0.6288)
0.9931426 0.00030991653
Epoch: [1960][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6623 (0.6280)
0.9771966 0.0021737174
loss:  0.865917766927009 0.7166599564597753
Epoch 1961: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1961][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6414 (0.6414)
0.99999213 1.7727203e-34
===========>   testing    <===========
Epoch: [1961][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5598 (0.5598)
0.99211395 6.7522375e-15
Epoch: [1961][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5308 (0.6312)
0.99268943 0.0023204365
Epoch: [1961][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6939 (0.6307)
0.9730926 0.0028931857
loss:  0.8640857732527705 0.7166599564597753
Epoch 1962: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1962][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6394 (0.6394)
0.99767846 3.014725e-06
===========>   testing    <===========
Epoch: [1962][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5696 (0.5696)
0.98988324 6.674161e-07
Epoch: [1962][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5620 (0.6405)
0.9955537 0.0019939658
Epoch: [1962][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6812 (0.6392)
0.98645025 0.002877037
loss:  0.8651781168825039 0.7166599564597753
Epoch 1963: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1963][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6332 (0.6332)
0.99414057 0.0029219086
===========>   testing    <===========
Epoch: [1963][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5487 (0.5487)
0.9926122 8.5423025e-08
Epoch: [1963][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5079 (0.6287)
0.99411124 0.0006583413
Epoch: [1963][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6774 (0.6294)
0.9778309 0.002229708
loss:  0.8598484047111965 0.7166599564597753
Epoch 1964: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1964][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6351 (0.6351)
0.99530715 3.517951e-05
===========>   testing    <===========
Epoch: [1964][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5566 (0.5566)
0.9945643 4.7467154e-08
Epoch: [1964][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5446 (0.6361)
0.9956874 0.0012213057
Epoch: [1964][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6639 (0.6334)
0.98441434 0.0020501434
loss:  0.8604962941780789 0.7166599564597753
Epoch 1965: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1965][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6545 (0.6545)
0.9943177 3.93746e-39
===========>   testing    <===========
Epoch: [1965][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5605 (0.5605)
0.9967529 8.1403607e-07
Epoch: [1965][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.4931 (0.6352)
0.99486274 0.002037716
Epoch: [1965][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6719 (0.6345)
0.98257357 0.0021689434
loss:  0.8672144152528306 0.7166599564597753
Epoch 1966: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1966][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6409 (0.6409)
0.99369955 0.00028268123
===========>   testing    <===========
Epoch: [1966][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5418 (0.5418)
0.99354714 1.5970762e-05
Epoch: [1966][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5128 (0.6266)
0.9937243 0.00015724063
Epoch: [1966][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6805 (0.6264)
0.98043114 0.00014387425
loss:  0.8559034859134049 0.7166599564597753
Epoch 1967: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1967][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6468 (0.6468)
0.995133 0.0011522059
===========>   testing    <===========
Epoch: [1967][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5646 (0.5646)
0.9940643 1.5070253e-14
Epoch: [1967][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5021 (0.6343)
0.99323785 0.0021908663
Epoch: [1967][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6694 (0.6334)
0.9858294 0.0074488795
loss:  0.860971780362195 0.7166599564597753
Epoch 1968: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1968][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6330 (0.6330)
0.99403375 0.0011221954
===========>   testing    <===========
Epoch: [1968][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5612 (0.5612)
0.99356985 3.142922e-15
Epoch: [1968][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5165 (0.6296)
0.993451 0.0008991726
Epoch: [1968][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6813 (0.6285)
0.9929941 0.0003976399
loss:  0.8599275679737409 0.7166599564597753
Epoch 1969: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1969][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6498 (0.6498)
0.97936696 0.0034054448
===========>   testing    <===========
Epoch: [1969][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5439 (0.5439)
0.9933664 6.9849326e-09
Epoch: [1969][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5586 (0.6333)
0.9943541 0.0008697478
Epoch: [1969][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6832 (0.6323)
0.9758186 0.0020352935
loss:  0.8627028093364628 0.7166599564597753
Epoch 1970: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1970][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6354 (0.6354)
0.9988494 4.2723556e-08
===========>   testing    <===========
Epoch: [1970][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5543 (0.5543)
0.99381244 2.7552006e-12
Epoch: [1970][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5193 (0.6298)
0.9934047 0.00014716868
Epoch: [1970][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6906 (0.6270)
0.98965704 4.2212334e-05
loss:  0.8698898102409122 0.7166599564597753
Epoch 1971: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1971][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6427 (0.6427)
0.9969597 1.7631994e-05
===========>   testing    <===========
Epoch: [1971][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5362 (0.5362)
0.9936488 1.7672805e-09
Epoch: [1971][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5290 (0.6305)
0.99288005 0.0012986392
Epoch: [1971][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6546 (0.6298)
0.97799826 0.002685499
loss:  0.8671397494192132 0.7166599564597753
Epoch 1972: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1972][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6366 (0.6366)
0.9920791 6.916397e-05
===========>   testing    <===========
Epoch: [1972][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5477 (0.5477)
0.9937929 3.1184037e-08
Epoch: [1972][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5050 (0.6334)
0.9944166 0.0020244978
Epoch: [1972][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6742 (0.6323)
0.9757463 0.0009066754
loss:  0.8638181918839891 0.7166599564597753
Epoch 1973: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1973][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6439 (0.6439)
0.9926197 2.9083378e-14
===========>   testing    <===========
Epoch: [1973][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5543 (0.5543)
0.9929806 3.402864e-14
Epoch: [1973][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5297 (0.6340)
0.99199283 3.723625e-05
Epoch: [1973][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6907 (0.6327)
0.9873963 0.006066092
loss:  0.8589758041799256 0.7166599564597753
Epoch 1974: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1974][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6378 (0.6378)
0.99277085 0.008982067
===========>   testing    <===========
Epoch: [1974][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5497 (0.5497)
0.9925815 2.0502874e-10
Epoch: [1974][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5148 (0.6264)
0.99374145 0.0021414696
Epoch: [1974][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6732 (0.6250)
0.9830951 0.0023046406
loss:  0.8601566563034201 0.7166599564597753
Epoch 1975: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1975][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6431 (0.6431)
0.9950913 0.0005103902
===========>   testing    <===========
Epoch: [1975][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5534 (0.5534)
0.9929497 1.7230759e-08
Epoch: [1975][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5282 (0.6321)
0.9937224 9.269676e-06
Epoch: [1975][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6708 (0.6325)
0.98505694 0.00037531724
loss:  0.8578503499651882 0.7166599564597753
Epoch 1976: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1976][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6218 (0.6218)
0.99630153 7.408518e-26
===========>   testing    <===========
Epoch: [1976][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5242 (0.5242)
0.99560153 2.5772502e-13
Epoch: [1976][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5112 (0.6295)
0.9954373 0.0001662999
Epoch: [1976][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6686 (0.6283)
0.9811382 0.0020056323
loss:  0.8608278074637657 0.7166599564597753
Epoch 1977: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1977][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6409 (0.6409)
0.99580246 0.000499918
===========>   testing    <===========
Epoch: [1977][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5522 (0.5522)
0.9910671 2.2314744e-10
Epoch: [1977][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5218 (0.6343)
0.9930569 0.001156377
Epoch: [1977][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6732 (0.6329)
0.9854081 0.0038309928
loss:  0.864824364182364 0.7166599564597753
Epoch 1978: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1978][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6488 (0.6488)
0.9883116 2.9865165e-13
===========>   testing    <===========
Epoch: [1978][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5424 (0.5424)
0.9924314 7.2182296e-08
Epoch: [1978][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.4825 (0.6275)
0.9940546 0.0022175785
Epoch: [1978][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6797 (0.6274)
0.9927637 0.001408906
loss:  0.8598236814194291 0.7166599564597753
Epoch 1979: LambdaDecay set learning rate to 1.0496229088680773e-07.
===========>   training    <===========
Epoch: [1979][0/6]      Lr: [1.0496229088680773e-07]    Loss 0.6506 (0.6506)
0.99365604 0.0018749563
===========>   testing    <===========
Epoch: [1979][0/289]    Lr: [1.0496229088680773e-07]    Loss 0.5358 (0.5358)
0.9937406 2.8988486e-08
Epoch: [1979][100/289]  Lr: [1.0496229088680773e-07]    Loss 0.5218 (0.6296)
0.99342006 0.0009944831
Epoch: [1979][200/289]  Lr: [1.0496229088680773e-07]    Loss 0.6745 (0.6297)
0.9802592 0.0019171825
loss:  0.8666860578361102 0.7166599564597753
Epoch 1980: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1980][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6435 (0.6435)
0.9934936 0.00018017758
===========>   testing    <===========
Epoch: [1980][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5513 (0.5513)
0.993899 3.437573e-08
Epoch: [1980][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5376 (0.6325)
0.99352515 0.0010265938
Epoch: [1980][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6644 (0.6301)
0.9813906 0.0024296863
loss:  0.858371944011427 0.7166599564597753
Epoch 1981: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1981][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6494 (0.6494)
0.9835164 3.3171846e-36
===========>   testing    <===========
Epoch: [1981][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5340 (0.5340)
0.9932034 2.3429679e-07
Epoch: [1981][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5018 (0.6302)
0.9994677 1.0219767e-06
Epoch: [1981][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6819 (0.6286)
0.9746429 0.00046653446
loss:  0.8652349255436591 0.7166599564597753
Epoch 1982: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1982][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6272 (0.6272)
0.9980312 1.7327277e-17
===========>   testing    <===========
Epoch: [1982][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5563 (0.5563)
0.9932672 3.1346885e-20
Epoch: [1982][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5252 (0.6352)
0.995274 0.00047283553
Epoch: [1982][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6965 (0.6339)
0.975066 0.008041259
loss:  0.8626070469774486 0.7166599564597753
Epoch 1983: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1983][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6409 (0.6409)
0.9979898 2.488389e-09
===========>   testing    <===========
Epoch: [1983][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5416 (0.5416)
0.9944239 1.226244e-09
Epoch: [1983][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5201 (0.6297)
0.99566406 0.001102434
Epoch: [1983][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6914 (0.6288)
0.9851576 0.00018833108
loss:  0.8618704291123742 0.7166599564597753
Epoch 1984: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1984][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6362 (0.6362)
0.9991886 3.600477e-16
===========>   testing    <===========
Epoch: [1984][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5611 (0.5611)
0.98613876 2.9623938e-13
Epoch: [1984][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5189 (0.6338)
0.99418795 1.2105478e-05
Epoch: [1984][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6740 (0.6331)
0.99159896 0.0022172534
loss:  0.8706340400830834 0.7166599564597753
Epoch 1985: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1985][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6361 (0.6361)
0.99528223 2.4371257e-05
===========>   testing    <===========
Epoch: [1985][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5498 (0.5498)
0.9936287 6.8775297e-10
Epoch: [1985][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5040 (0.6254)
0.993544 0.0023934313
Epoch: [1985][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6810 (0.6241)
0.98940164 0.002276855
loss:  0.8620903746599387 0.7166599564597753
Epoch 1986: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1986][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6409 (0.6409)
0.994221 6.4470294e-07
===========>   testing    <===========
Epoch: [1986][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5540 (0.5540)
0.9953986 8.7993875e-16
Epoch: [1986][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5411 (0.6362)
0.994936 0.00074033695
Epoch: [1986][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6665 (0.6325)
0.9897664 0.0024171683
loss:  0.8607310880779704 0.7166599564597753
Epoch 1987: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1987][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6403 (0.6403)
0.97457135 1.9716144e-05
===========>   testing    <===========
Epoch: [1987][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5673 (0.5673)
0.9937111 1.9880794e-07
Epoch: [1987][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5321 (0.6359)
0.99429387 0.002092738
Epoch: [1987][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6678 (0.6346)
0.99437225 0.0025501822
loss:  0.8635064755422602 0.7166599564597753
Epoch 1988: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1988][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6303 (0.6303)
1.0 3.345855e-29
===========>   testing    <===========
Epoch: [1988][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5494 (0.5494)
0.9932662 2.3056989e-16
Epoch: [1988][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5428 (0.6283)
0.99295115 0.0012350585
Epoch: [1988][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6813 (0.6271)
0.97597355 0.0023803962
loss:  0.8592773125765907 0.7166599564597753
Epoch 1989: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1989][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6367 (0.6367)
0.98544437 0.0043480597
===========>   testing    <===========
Epoch: [1989][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5474 (0.5474)
0.9945379 7.5642577e-14
Epoch: [1989][100/289]  Lr: [9.971417634246734e-08]     Loss 0.4918 (0.6312)
0.99487066 0.0005119421
Epoch: [1989][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6632 (0.6302)
0.98909646 0.0026256244
loss:  0.8615836664610571 0.7166599564597753
Epoch 1990: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1990][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6438 (0.6438)
0.9744434 0.0014988112
===========>   testing    <===========
Epoch: [1990][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5587 (0.5587)
0.99376243 4.773227e-15
Epoch: [1990][100/289]  Lr: [9.971417634246734e-08]     Loss 0.4972 (0.6339)
0.99665403 0.00031298265
Epoch: [1990][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6886 (0.6318)
0.9921278 0.0010654721
loss:  0.8641313861343319 0.7166599564597753
Epoch 1991: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1991][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6298 (0.6298)
0.9953969 0.0014939635
===========>   testing    <===========
Epoch: [1991][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5588 (0.5588)
0.99356353 1.7531737e-12
Epoch: [1991][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5304 (0.6323)
0.99420667 0.0019624655
Epoch: [1991][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6661 (0.6315)
0.98715436 0.007317217
loss:  0.8629577715494919 0.7166599564597753
Epoch 1992: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1992][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6313 (0.6313)
0.9737472 0.003276701
===========>   testing    <===========
Epoch: [1992][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5495 (0.5495)
0.9938791 2.8958405e-07
Epoch: [1992][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5079 (0.6348)
0.99332595 0.0011584504
Epoch: [1992][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6576 (0.6332)
0.99241686 0.00010759582
loss:  0.8629028829074293 0.7166599564597753
Epoch 1993: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1993][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6341 (0.6341)
0.9706683 0.005743612
===========>   testing    <===========
Epoch: [1993][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5409 (0.5409)
0.99590576 9.301523e-07
Epoch: [1993][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5084 (0.6348)
0.994255 0.0008099125
Epoch: [1993][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6539 (0.6327)
0.99102193 0.0021687245
loss:  0.8566898349287584 0.7166599564597753
Epoch 1994: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1994][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6363 (0.6363)
0.9882908 5.260911e-09
===========>   testing    <===========
Epoch: [1994][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5391 (0.5391)
0.99325186 1.3363855e-07
Epoch: [1994][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5224 (0.6270)
0.9940666 0.00014417034
Epoch: [1994][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6629 (0.6255)
0.99412364 0.001119182
loss:  0.8612049838692013 0.7166599564597753
Epoch 1995: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1995][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6313 (0.6313)
0.99842274 0.00012551979
===========>   testing    <===========
Epoch: [1995][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5647 (0.5647)
0.9924919 9.139109e-05
Epoch: [1995][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5357 (0.6311)
0.9936546 0.00090241607
Epoch: [1995][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6778 (0.6302)
0.9926965 0.0012591159
loss:  0.8627980499312704 0.7166599564597753
Epoch 1996: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1996][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6354 (0.6354)
0.9953629 0.0003955081
===========>   testing    <===========
Epoch: [1996][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5495 (0.5495)
0.9944184 1.2468546e-05
Epoch: [1996][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5369 (0.6355)
0.99407697 0.0007579351
Epoch: [1996][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6805 (0.6341)
0.9823913 0.0022115873
loss:  0.8620216878133704 0.7166599564597753
Epoch 1997: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1997][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6358 (0.6358)
0.99223846 0.0018276019
===========>   testing    <===========
Epoch: [1997][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5359 (0.5359)
0.9921216 6.8240046e-11
Epoch: [1997][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5000 (0.6303)
0.9955063 0.00059039134
Epoch: [1997][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6718 (0.6303)
0.98767394 0.0018657765
loss:  0.8554124266972429 0.7166599564597753
Epoch 1998: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1998][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6385 (0.6385)
0.9785824 0.0014378444
===========>   testing    <===========
Epoch: [1998][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5547 (0.5547)
0.99365646 9.86711e-06
Epoch: [1998][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5278 (0.6287)
0.99378127 0.0022574232
Epoch: [1998][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6618 (0.6276)
0.9805565 0.0016523344
loss:  0.8628226386445096 0.7166599564597753
Epoch 1999: LambdaDecay set learning rate to 9.971417634246734e-08.
===========>   training    <===========
Epoch: [1999][0/6]      Lr: [9.971417634246734e-08]     Loss 0.6441 (0.6441)
0.9721734 0.0025232357
===========>   testing    <===========
Epoch: [1999][0/289]    Lr: [9.971417634246734e-08]     Loss 0.5597 (0.5597)
0.9934263 1.1236582e-13
Epoch: [1999][100/289]  Lr: [9.971417634246734e-08]     Loss 0.5363 (0.6295)
0.9933722 0.0010763155
Epoch: [1999][200/289]  Lr: [9.971417634246734e-08]     Loss 0.6525 (0.6296)
0.9849215 0.0017618127
loss:  0.8645680427707725 0.7166599564597753
Epoch 2000: LambdaDecay set learning rate to 9.472846752534397e-08.